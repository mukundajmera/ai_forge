# PiSSA + QLoRA Training Configuration
# Optimized for Apple Silicon (M1/M2/M3) with Unified Memory
#
# THEORY:
# PiSSA (Principal singular components Initialization for PEFT): 
#   Instead of random Gaussian initialization (LoRA), PiSSA initializes
#   adapter matrices using principal singular components via SVD.
#   Benefits: 3-5x faster convergence, +5% accuracy on code benchmarks.
#
# QLoRA (Quantized LoRA):
#   Freezes base model in 4-bit NF4 precision, trains only adapters.
#   Reduces memory by 75%, enabling 7B model training on 8GB RAM.

# =============================================================================
# Model Configuration
# =============================================================================
model:
  # Base model from Unsloth (optimized for Mac)
  base_model: "unsloth/Llama-3.2-3B-Instruct-4bit"
  
  # Maximum sequence length (context window)
  max_seq_length: 2048
  
  # Model dtype (auto-detect for Apple Silicon)
  dtype: "auto"
  
  # Load in 4-bit quantized format
  load_in_4bit: true
  
  # Trust remote code (for custom architectures)
  trust_remote_code: true

# =============================================================================
# PiSSA Configuration (Key Innovation)
# =============================================================================
pissa:
  # Initialization method: "pissa" (SVD-based) vs "gaussian" (standard LoRA)
  init_method: "pissa"
  
  # Rank of adapter matrices
  # Higher ranks capture more information but use more memory
  # PiSSA can use higher ranks stably (64-128) vs LoRA (16-32)
  rank: 64
  
  # Scaling factor (typically 2x rank)
  # Higher alpha = stronger adapter influence
  lora_alpha: 128
  
  # Dropout for regularization
  lora_dropout: 0.05
  
  # Target modules to adapt
  # These are the attention projection matrices
  target_modules:
    - "q_proj"  # Query projection
    - "k_proj"  # Key projection
    - "v_proj"  # Value projection
    - "o_proj"  # Output projection
    # Optional: Include MLP layers for more capacity
    # - "gate_proj"
    # - "up_proj"
    # - "down_proj"
  
  # Use RSLoRA scaling (divide by sqrt(rank))
  # Improves stability at higher ranks
  use_rslora: true
  
  # PiSSA-specific: Number of iterations for SVD refinement
  pissa_niter: 4

# =============================================================================
# Quantization Configuration
# =============================================================================
quantization:
  # Quantization bits (4-bit is optimal for Mac)
  bits: 4
  
  # Quantization type
  # "nf4": Normal Float 4-bit (best for most cases)
  # "fp4": Pure 4-bit floating point
  quant_type: "nf4"
  
  # Double quantization (quantize the quantization constants)
  # Reduces memory by ~0.3GB with minimal quality loss
  double_quant: true
  
  # Compute dtype for matrix multiplications
  compute_dtype: "bfloat16"

# =============================================================================
# Training Configuration
# =============================================================================
training:
  # Learning rate (PiSSA works well with higher LR than LoRA)
  learning_rate: 2e-4
  
  # Learning rate scheduler
  lr_scheduler_type: "cosine"
  
  # Warmup steps (or ratio)
  warmup_steps: 100
  warmup_ratio: 0.03
  
  # Weight decay for regularization
  weight_decay: 0.01
  
  # Maximum gradient norm (gradient clipping)
  max_grad_norm: 1.0
  
  # Optimizer: 8-bit AdamW for memory efficiency
  optimizer: "adamw_8bit"
  
  # Batch size per device
  # Mac optimized: 1-4 depending on model size and RAM
  per_device_train_batch_size: 2
  per_device_eval_batch_size: 2
  
  # Gradient accumulation
  # Effective batch = batch_size * gradient_accumulation_steps
  # Higher values = more stable training, less memory
  gradient_accumulation_steps: 4
  
  # Number of training epochs
  num_train_epochs: 3
  
  # Maximum training steps (overrides epochs if set)
  max_steps: -1  # -1 means use epochs
  
  # Seed for reproducibility
  seed: 42

# =============================================================================
# Evaluation Configuration
# =============================================================================
evaluation:
  # Evaluation strategy: "steps", "epoch", or "no"
  strategy: "steps"
  
  # Evaluate every N steps (if strategy is "steps")
  eval_steps: 50
  
  # Number of predictions to log
  eval_accumulation_steps: 4

# =============================================================================
# Checkpointing Configuration
# =============================================================================
checkpoints:
  # Save strategy: "steps", "epoch", or "no"
  save_strategy: "steps"
  
  # Save every N steps
  save_steps: 100
  
  # Keep only last N checkpoints
  save_total_limit: 3
  
  # Load best model at end of training
  load_best_model_at_end: true
  
  # Metric for best model selection
  metric_for_best_model: "eval_loss"
  
  # Greater is better for metric
  greater_is_better: false
  
  # Resume from checkpoint path (null = start fresh)
  resume_from_checkpoint: null

# =============================================================================
# Early Stopping Configuration
# =============================================================================
early_stopping:
  # Enable early stopping
  enabled: true
  
  # Patience: number of evaluations without improvement before stopping
  patience: 5
  
  # Minimum delta: minimum change to qualify as improvement
  min_delta: 0.001
  
  # Metric to monitor
  metric: "eval_loss"

# =============================================================================
# Memory Management (Critical for Mac)
# =============================================================================
memory:
  # Memory alert threshold (fraction of total)
  alert_threshold: 0.80
  
  # Enable gradient checkpointing (trades compute for memory)
  gradient_checkpointing: true
  
  # Use CPU offload for optimizer states
  cpu_offload: false
  
  # Empty cache between batches
  empty_cache_steps: 10
  
  # Maximum memory fraction to use
  max_memory_fraction: 0.90

# =============================================================================
# Logging Configuration
# =============================================================================
logging:
  # Logging steps
  logging_steps: 10
  
  # Log to console and file
  log_level: "info"
  
  # Output directory for logs
  output_dir: "./output"
  
  # Run name for tracking
  run_name: null  # Auto-generated if null
  
  # Report to (tensorboard, wandb, etc.)
  report_to: "tensorboard"

# =============================================================================
# DPO Configuration (Optional Second Phase)
# =============================================================================
dpo:
  # Enable DPO phase after SFT
  enabled: false
  
  # Beta parameter (KL penalty strength)
  beta: 0.1
  
  # Learning rate for DPO (typically lower than SFT)
  learning_rate: 5e-5
  
  # Number of DPO epochs
  num_epochs: 1
  
  # DPO loss type: "sigmoid", "hinge", "ipo"
  loss_type: "sigmoid"

# =============================================================================
# Hardware Detection (Auto-configured at runtime)
# =============================================================================
hardware:
  # Device (auto-detect: cuda, mps, cpu)
  device: "auto"
  
  # Mixed precision training
  fp16: false
  bf16: true  # Preferred for Apple Silicon
  
  # Number of workers for data loading
  dataloader_num_workers: 0  # 0 for Mac (no multiprocessing overhead)
